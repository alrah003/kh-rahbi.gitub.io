---
title: "Data Engineering Bootcamp"
description: "building a real-time dashboard for 300M NYC-Taxi Trips"
dateString: May 2023
draft: false
tags: ["Data Engineering", "Google Cloud Platform", "GCP", "BigQuery", "Terraform", "Docker", "SQL", "Prefect", "dbt", "Spark", "Kafka"]
showToc: false
weight: [Weight of the project in my portfolio]
cover:
    image: "projects/DEZC/cover1.png"
--- 
## Description

The Data Engineering Bootcamp was a comprehensive program that equipped me with the necessary skills and knowledge to excel in the field of data engineering. Throughout the bootcamp, I worked extensively with cutting-edge technologies and platforms to develop robust data pipelines and infrastructure.

### Technologies

- **Google Cloud Platform (GCP)**: I leveraged GCP as my cloud-based auto-scaling platform, providing a scalable and flexible infrastructure for my data engineering projects.
- **Google Cloud Storage (GCS)**: GCS served as my data lake, enabling efficient storage and retrieval of large volumes of data.
- **BigQuery**: As my data warehouse solution, BigQuery empowered me to analyze and query vast datasets quickly and effectively.
- **Terraform**: I utilized Terraform as my Infrastructure-as-Code (IaC) tool to provision and manage my cloud resources and services.
- **Docker**: Docker played a crucial role in containerizing my applications and ensuring consistent deployment across different environments.
- **SQL**: SQL was instrumental in performing data analysis and exploration, allowing me to extract valuable insights from structured datasets.
- **Prefect**: I employed Prefect as my workflow orchestration tool to create and manage complex data pipelines with ease and reliability.
- **dbt**: dbt served as my data transformation tool, enabling me to perform advanced data manipulations and create refined datasets for downstream analysis and reporting.
- **Spark**: Spark, a distributed processing framework, enabled me to process large-scale datasets efficiently and perform complex data transformations.
- **Kafka**: I utilized Kafka for real-time streaming data processing, allowing me to build robust and scalable streaming pipelines.

By gaining hands-on experience with these technologies, I developed a deep understanding of modern data engineering practices and acquired the skills to design and implement scalable, reliable, and efficient data pipelines. Throughout the bootcamp, I collaborated on various projects, tackling real-world data engineering challenges and delivering impactful solutions.

The Data Engineering Bootcamp not only expanded my technical expertise but also fostered a strong problem-solving mindset and effective collaboration within the team. This experience has equipped me to excel in the dynamic and rapidly evolving field of data engineering, ready to tackle complex data challenges and drive data-centric innovation.

![](/projects/DEZC/result.webp)